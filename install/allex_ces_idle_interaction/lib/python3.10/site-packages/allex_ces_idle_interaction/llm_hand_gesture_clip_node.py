#!/usr/bin/env python3
"""
LLM Publisher Node - CLIP ì•™ìƒë¸” ì¶”ë¡  + RUN/STOP ì œì–´
v1.2.0 - GUIì—ì„œ RUN ëª…ë ¹ì„ ë°›ìœ¼ë©´ ì¶”ë¡  ì‹œì‘, STOP ëª…ë ¹ì„ ë°›ìœ¼ë©´ ëŒ€ê¸°

ì‹œìŠ¤í…œ êµ¬ì¡°:
- SPARK 1 PC: Camera Publisher (ì¹´ë©”ë¼ + YOLO ì¶”ì ) â†’ /allex_camera/target_crop/compressed ë°œí–‰
- SPARK 2 PC: LLM Publisher (ì´ ë…¸ë“œ) â†’ /allex_camera/target_crop/compressed êµ¬ë…, /llm/response ë°œí–‰
- Laptop: GUI â†’ /llm/control ë°œí–‰í•˜ì—¬ LLM Publisher ì œì–´
"""

import torch
import time
import json
from PIL import Image
import numpy as np

import rclpy
from rclpy.node import Node
from rclpy.qos import QoSProfile, ReliabilityPolicy, HistoryPolicy, DurabilityPolicy, Duration
from sensor_msgs.msg import CompressedImage
from std_msgs.msg import String
import cv2

from concurrent.futures import ThreadPoolExecutor

# --- ì„¤ì • ---
MODEL_IDS = [
    "openai/clip-vit-large-patch14-336",
    #"facebook/metaclip-h14-fullcc2.5b"
]
USE_FP16 = True
TARGET_INFER_HZ = 7  # ëª©í‘œ ì¶”ë¡  Hz

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìš© QoS
REALTIME_QOS = QoSProfile(
    reliability=ReliabilityPolicy.BEST_EFFORT,
    durability=DurabilityPolicy.VOLATILE,
    history=HistoryPolicy.KEEP_LAST,
    depth=1
)

# ì œì–´ ëª…ë ¹ìš© QoS (ì‹ ë¢°ì„± ë³´ì¥)
RELIABLE_QOS = QoSProfile(
    depth=10,
    reliability=ReliabilityPolicy.RELIABLE,
    deadline=Duration(seconds=0, nanoseconds=0),
)

# --- ë¶„ë¥˜ ë¼ë²¨ ---
# ê° ì œìŠ¤ì²˜(handshake, highfive, fist)ì— ëŒ€í•´ ì„œë¸Œë¼ë²¨ 3ê°œì”© ì •ì˜
LABELS = [
    # handshake (3ê°œ ì„œë¸Œë¼ë²¨)
    "a person reaching their hand forward to shake hands, hand near waist height, fingers relaxed",
    "a person extending their right hand forward for a handshake, arm slightly bent",
    "a close-up of two hands about to shake, fingers open and relaxed",
    # highfive (3ê°œ ì„œë¸Œë¼ë²¨)
    "a person raising one hand high above their head for a high five, palm open",
    "a person leaning forward with hand up for a high five, palm facing forward",
    "two people with one hand each in the air about to high five",
    # fist bump (3ê°œ ì„œë¸Œë¼ë²¨)
    "a person extending a closed fist forward for a fist bump",
    "two fists meeting in a fist bump gesture",
    "a person holding a clenched fist out in front of them for a friendly bump",
    # idle (1ê°œ ë¼ë²¨)
    "a person standing normally with hands down and no interaction"
]
SHORT_LABELS = ("handshake", "highfive", "fist", "idle")


def check_gpu_status():
    """GPU ìƒíƒœ í™•ì¸ ë° ì¶œë ¥"""
    print("=" * 60)
    print("LLM Publisher - GPU ìƒíƒœ í™•ì¸")
    print("=" * 60)
    
    print(f"PyTorch ë²„ì „: {torch.__version__}")
    print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA ë²„ì „: {torch.version.cuda}")
        print(f"cuDNN ë²„ì „: {torch.backends.cudnn.version()}")
        print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"  GPU {i}: {props.name}")
            print(f"    ë©”ëª¨ë¦¬: {props.total_memory / 1024**3:.1f} GB")
        
        print(f"í˜„ì¬ GPU: {torch.cuda.current_device()}")
        print(f"í˜„ì¬ GPU ì´ë¦„: {torch.cuda.get_device_name()}")
    else:
        print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤! CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    print("=" * 60)


class LLMPublisher(Node):
    """LLM Publisher Node - CLIP ì•™ìƒë¸” ì¶”ë¡  + RUN/STOP ì œì–´"""
    
    def __init__(self) -> None:
        super().__init__('llm_publisher')
        
        # GPU ìƒíƒœ í™•ì¸
        check_gpu_status()
        
        # ì‹¤í–‰ ìƒíƒœ í”Œë˜ê·¸
        self.is_running = False
        
        # í† í”½ ì´ë¦„ (topics.jsonì—ì„œ ê°€ì ¸ì™€ë„ ë˜ì§€ë§Œ, ë…ë¦½ ì‹¤í–‰ì„ ìœ„í•´ í•˜ë“œì½”ë”©)
        self.image_topic = "/allex_camera/target_crop/compressed"
        self.result_topic = "/llm/response"
        self.control_topic = "/llm/control"
        
        # ì´ë¯¸ì§€ êµ¬ë… (ì‹¤ì‹œê°„ QoS)
        self.image_subscription = self.create_subscription(
            CompressedImage,
            self.image_topic,
            self._image_callback,
            REALTIME_QOS
        )
        
        # ê²°ê³¼ ë°œí–‰
        self.result_publisher = self.create_publisher(
            String,
            self.result_topic,
            REALTIME_QOS
        )
        
        # ì œì–´ ëª…ë ¹ êµ¬ë… (GUIì—ì„œ RUN/STOP)
        self.control_subscription = self.create_subscription(
            String,
            self.control_topic,
            self._control_callback,
            RELIABLE_QOS
        )
        
        # ìƒíƒœ ë°œí–‰ (GUIì— í˜„ì¬ ìƒíƒœ ì•Œë¦¼)
        self.status_publisher = self.create_publisher(
            String,
            "/llm/status",
            10
        )
        
        # CLIP ëª¨ë¸ ë¡œë“œ
        self.get_logger().info("ğŸ“¦ CLIP ëª¨ë¸ ë¡œë“œ ì¤‘...")
        self._load_models()
        
        # ë³‘ë ¬ ì‹¤í–‰ìš© ThreadPool
        self.thread_pool = ThreadPoolExecutor(max_workers=len(MODEL_IDS))
        
        # ì¶”ë¡  ì œì–´
        self.last_infer_time = 0
        self.min_infer_interval = 1.0 / TARGET_INFER_HZ
        self.infer_hz = 0.0
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.frame_count = 0
        self.last_log_time = time.monotonic()
        
        # ì£¼ê¸°ì  ìƒíƒœ ë°œí–‰ íƒ€ì´ë¨¸ (1ì´ˆë§ˆë‹¤)
        self.status_timer = self.create_timer(1.0, self._publish_status)
        
        self.get_logger().info("=" * 60)
        self.get_logger().info("âœ… LLM Publisher ì´ˆê¸°í™” ì™„ë£Œ!")
        self.get_logger().info(f"ğŸ“¡ ì´ë¯¸ì§€ êµ¬ë…: {self.image_topic}")
        self.get_logger().info(f"ğŸ“¡ ê²°ê³¼ ë°œí–‰: {self.result_topic}")
        self.get_logger().info(f"ğŸ“¡ ì œì–´ êµ¬ë…: {self.control_topic}")
        self.get_logger().info(f"âš™ï¸  ëª©í‘œ Hz: {TARGET_INFER_HZ}")
        self.get_logger().info(f"ğŸ”€ ë³‘ë ¬ ì²˜ë¦¬: {len(MODEL_IDS)}ê°œ ëª¨ë¸")
        self.get_logger().info("=" * 60)
        self.get_logger().info("â¸ï¸  ëŒ€ê¸° ì¤‘: RUN ëª…ë ¹ì„ ê¸°ë‹¤ë¦½ë‹ˆë‹¤...")
    
    def _load_models(self):
        """CLIP ëª¨ë¸ ë¡œë“œ"""
        from transformers import CLIPProcessor, CLIPModel
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.get_logger().info(f"ğŸ–¥ï¸  Device: {self.device}")
        
        self.models = []
        self.processors = []
        self.streams = []  # CUDA Streams
        
        for model_id in MODEL_IDS:
            self.get_logger().info(f"")
            self.get_logger().info(f"{'='*50}")
            self.get_logger().info(f"ğŸ§  Loading: {model_id}")
            self.get_logger().info(f"{'='*50}")
            
            try:
                # ë¨¼ì € ë¡œì»¬ ìºì‹œì—ì„œ ë¡œë“œ ì‹œë„
                model = CLIPModel.from_pretrained(model_id, local_files_only=True).to(self.device)
                processor = CLIPProcessor.from_pretrained(model_id, local_files_only=True)
            except Exception:
                # ë¡œì»¬ì— ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ
                self.get_logger().info(f"   ë¡œì»¬ ìºì‹œì— ì—†ìŒ, ë‹¤ìš´ë¡œë“œ ì¤‘...")
                model = CLIPModel.from_pretrained(model_id).to(self.device)
                processor = CLIPProcessor.from_pretrained(model_id)
            
            if USE_FP16 and self.device == "cuda":
                model = model.half()
            
            model.eval()
            self.models.append(model)
            self.processors.append(processor)
            
            # ê° ëª¨ë¸ìš© CUDA Stream ìƒì„±
            if self.device == "cuda":
                self.streams.append(torch.cuda.Stream())
            
            self.get_logger().info(f"âœ… {model_id} ë¡œë“œ ì™„ë£Œ!")
        
        self.get_logger().info(f"")
        self.get_logger().info(f"ğŸ¯ ì´ {len(self.models)}ê°œ ëª¨ë¸ ë³‘ë ¬ ì•™ìƒë¸” ì¤€ë¹„ ì™„ë£Œ!")
    
    def _control_callback(self, msg: String):
        """ì œì–´ ëª…ë ¹ ì½œë°± - GUIì—ì„œ RUN/STOP"""
        try:
            command = json.loads(msg.data)
            cmd_type = command.get('type', '')
            
            if cmd_type == 'run' or cmd_type == 'start':
                if not self.is_running:
                    self.is_running = True
                    self.get_logger().info("â–¶ï¸  RUN ëª…ë ¹ ìˆ˜ì‹  - ì¶”ë¡  ì‹œì‘!")
                    self._publish_status()
            
            elif cmd_type == 'stop':
                if self.is_running:
                    self.is_running = False
                    self.get_logger().info("â¹ï¸  STOP ëª…ë ¹ ìˆ˜ì‹  - ì¶”ë¡  ì¤‘ì§€")
                    self._publish_status()
            
            elif cmd_type == 'status':
                # ìƒíƒœ ìš”ì²­
                self._publish_status()
            
            else:
                self.get_logger().warn(f"ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {cmd_type}")
                
        except json.JSONDecodeError as e:
            self.get_logger().error(f"ì œì–´ ëª…ë ¹ íŒŒì‹± ì‹¤íŒ¨: {e}")
        except Exception as e:
            self.get_logger().error(f"ì œì–´ ëª…ë ¹ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _publish_status(self):
        """í˜„ì¬ ìƒíƒœë¥¼ ë°œí–‰"""
        try:
            status = {
                'running': self.is_running,
                'hz': round(self.infer_hz, 1),
                'models_loaded': len(self.models),
                'device': self.device,
                'timestamp': time.monotonic()
            }
            
            msg = String()
            msg.data = json.dumps(status)
            self.status_publisher.publish(msg)
            
        except Exception as e:
            self.get_logger().error(f"ìƒíƒœ ë°œí–‰ ì‹¤íŒ¨: {e}")
    
    def _image_callback(self, msg: CompressedImage):
        """ì´ë¯¸ì§€ ì½œë°± - RUN ìƒíƒœì¼ ë•Œë§Œ ì¶”ë¡ """
        # RUN ìƒíƒœê°€ ì•„ë‹ˆë©´ ë¬´ì‹œ
        if not self.is_running:
            return
        
        curr_time = time.time()
        time_since_last = curr_time - self.last_infer_time
        
        # Hz ì œí•œ
        if time_since_last < self.min_infer_interval:
            return
        
        try:
            # CompressedImage â†’ OpenCV ì´ë¯¸ì§€
            np_arr = np.frombuffer(msg.data, np.uint8)
            frame = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)
            
            if frame is None:
                return
            
            # BGR â†’ RGB ë³€í™˜ í›„ PIL Image
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            pil_image = Image.fromarray(frame_rgb)
            
            # ë³‘ë ¬ ì¶”ë¡  ì‹¤í–‰
            futures = []
            for idx in range(len(self.models)):
                future = self.thread_pool.submit(self._infer_single_model, idx, pil_image)
                futures.append(future)
            
            # ê²°ê³¼ ìˆ˜ì§‘ ë° ì•™ìƒë¸”
            num_sub_labels = len(LABELS)  # 10ê°œ (3+3+3+1)
            sublabel_probs = [0.0] * num_sub_labels
            for future in futures:
                model_probs = future.result()
                for i in range(num_sub_labels):
                    sublabel_probs[i] += model_probs[i] * 100
            
            # ì„œë¸Œë¼ë²¨ â†’ ë©”ì¸ ë¼ë²¨ 4ê°œë¡œ ì§‘ê³„
            ensemble_probs = [0.0] * 4
            ensemble_probs[0] = sublabel_probs[0] + sublabel_probs[1] + sublabel_probs[2]  # handshake
            ensemble_probs[1] = sublabel_probs[3] + sublabel_probs[4] + sublabel_probs[5]  # highfive
            ensemble_probs[2] = sublabel_probs[6] + sublabel_probs[7] + sublabel_probs[8]  # fist
            ensemble_probs[3] = sublabel_probs[9]  # idle
            
            # CUDA ë™ê¸°í™”
            if self.device == "cuda":
                torch.cuda.synchronize()
            
            # ì»¤ìŠ¤í…€ best ê²°ì • ë¡œì§
            IDLE_IDX = 3
            IDLE_THRESHOLD = 70.0
            
            if ensemble_probs[IDLE_IDX] >= IDLE_THRESHOLD:
                best_idx = IDLE_IDX
            else:
                non_idle_probs = ensemble_probs[:IDLE_IDX]
                best_idx = non_idle_probs.index(max(non_idle_probs))
            
            # Hz ê³„ì‚°
            if self.last_infer_time > 0:
                self.infer_hz = 1.0 / (curr_time - self.last_infer_time)
            self.last_infer_time = curr_time
            
            # ê²°ê³¼ ë°œí–‰
            result = {
                "best": SHORT_LABELS[best_idx],
                "probs": {SHORT_LABELS[i]: round(ensemble_probs[i], 1) for i in range(4)},
                "hz": round(self.infer_hz, 1),
                "timestamp": curr_time
            }
            
            result_msg = String()
            result_msg.data = json.dumps(result)
            self.result_publisher.publish(result_msg)
            
            # í”„ë ˆì„ ì¹´ìš´íŠ¸ ë° ì£¼ê¸°ì  ë¡œê·¸
            self.frame_count += 1
            current_time = time.monotonic()
            if current_time - self.last_log_time > 5.0:
                elapsed = current_time - self.last_log_time
                fps = self.frame_count / elapsed if elapsed > 0 else 0
                
                gpu_mem_str = ""
                if torch.cuda.is_available():
                    allocated = torch.cuda.memory_allocated() / 1024**2
                    gpu_mem_str = f" | GPU ë©”ëª¨ë¦¬: {allocated:.0f}MB"
                
                self.get_logger().info(
                    f"ğŸ§  ì¶”ë¡  ì¤‘: {SHORT_LABELS[best_idx]} | "
                    f"Hz: {self.infer_hz:.1f}{gpu_mem_str}"
                )
                self.frame_count = 0
                self.last_log_time = current_time
            
        except Exception as e:
            self.get_logger().error(f"ì¶”ë¡  ì˜¤ë¥˜: {e}")
    
    def _infer_single_model(self, idx, pil_image):
        """ë‹¨ì¼ ëª¨ë¸ ì¶”ë¡  (ë³‘ë ¬ ì‹¤í–‰ìš©)"""
        model = self.models[idx]
        processor = self.processors[idx]
        
        # CUDA Stream ì‚¬ìš©
        if self.device == "cuda" and idx < len(self.streams):
            stream = self.streams[idx]
            with torch.cuda.stream(stream):
                return self._run_inference(model, processor, pil_image)
        else:
            return self._run_inference(model, processor, pil_image)
    
    def _run_inference(self, model, processor, pil_image):
        """ì‹¤ì œ ì¶”ë¡  ìˆ˜í–‰"""
        inputs = processor(
            text=LABELS,
            images=pil_image,
            return_tensors="pt",
            padding=True
        ).to(self.device)
        
        if USE_FP16 and self.device == "cuda":
            inputs = {k: v.half() if v.dtype == torch.float32 else v 
                     for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        probs = outputs.logits_per_image.softmax(dim=1)
        return probs[0].cpu().tolist()
    
    def destroy_node(self):
        """ë…¸ë“œ ì¢…ë£Œ ì‹œ ì •ë¦¬"""
        self.thread_pool.shutdown(wait=False)
        super().destroy_node()


def main(args=None):
    """ë©”ì¸ í•¨ìˆ˜"""
    rclpy.init(args=args)
    llm_publisher = LLMPublisher()
    
    try:
        rclpy.spin(llm_publisher)
    except KeyboardInterrupt:
        llm_publisher.get_logger().info("\nâ¹ï¸ LLM Publisher ì¢…ë£Œ")
    finally:
        llm_publisher.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()


